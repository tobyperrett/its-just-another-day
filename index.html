<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="It's Just Another Day">
  <title>It's Just Another Day</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="container">
    <!-- Paper Title Section -->
    <header>
      <h1>It's Just Another Day: Unique Video Captioning by Discriminitave Prompting</h1>
      <h3 class="conference-tag">ACCV 2024 Oral</h3>
      <div class="authors">
        <h3>
          <a href="https://tobyperrett.github.io" target="_blank">Toby Perrett</a>, 
          <a href="https://tengdahan.github.io" target="_blank">Tengda Han</a>, 
          <a href="https://dimadamen.github.io" target="_blank">Dima Damen</a>, 
          <a href="https://www.robots.ox.ac.uk/~az/" target="_blank">Andrew Zisserman</a>
        </h3>
        <h4>
          <a href="https://arxiv.org/" target="_blank">arXiv</a> |
          <a href="https://github.com/" target="_blank">GitHub</a>
        </h4>
      </div>
    </header>

    <!-- Abstract Section -->
    <section id="introduction">
      <h2>Introduction</h2>
      <p>This paper investigates unique video captioning. We introduce a method, Captioning by Discriminitave Prompting (CDP) and challenging unique captioning benchmarks on Egocentric video and Timeloop movies.
      </p>
    </section>

    <!-- Figure Section -->
    <section id="problem statement">
      <h2>Problem statement</h2>

      <p>The following figures highlight a shortcoming of current captioning approaches. They caption each clip independently, giving similar captions for similar clips. First, in a timeloop movie:</p>

      <div class="figure-part-a">
        <img src="figs/tl.gif" alt="Part (a)" class="full-width-image">
        <!-- <img src="figs/egolong.png" alt="Part (a)" class="full-width-image"> -->
        <!-- <p><em>(a)A long egocentric video. The yellow/green/pink clips are initially captioned as “Opens the fridge” as existing methods caption clips independently.</em></p> -->
      </div>

      <p>Second, in a long egocentric video:</p>

      <!-- Part (a) -->
      <div class="figure-part-a">
        <img src="figs/e.gif" alt="Part (a)" class="full-width-image">
        <!-- <img src="figs/egolong.png" alt="Part (a)" class="full-width-image"> -->
        <!-- <p><em>(a)A long egocentric video. The yellow/green/pink clips are initially captioned as “Opens the fridge” as existing methods caption clips independently.</em></p> -->
      </div>


      <p>Given a set of similar video clips, our goal is to generate a caption for each, which is concise and focuses on what is unique to each clip. This should give a one-to-one relationship between clips and captions, were each caption can retrieve its corresponding video clip. We visualise our approach here: </p>
        

      <!-- Parts (b), (c), and (d) -->
      <div class="figure-row">
        <div class="figure-part">
          <img src="figs/t1.png" alt="Part (b)">
          <p><em>(a) Standard captioning can generate the same caption for multiple clips.</em></p>
        </div>
        <div class="figure-part">
          <img src="figs/t2.png" alt="Part (c)">
          <p><em>(b) We consider clips with the same caption, to find a property that captions them uniquely. Here, what the person is "holding" can uniquely identify the pink clip.</em></p>
        </div>
        <div class="figure-part">
          <img src="figs/t3.png" alt="Part (d)">
          <p><em>(c) If we cannot find a unique property, we explore the following clips for an extended unique caption.</em></p>
        </div>
      </div>


    </section>

    <!-- Introduction Section -->
    <section id="method">
      <h2>Method</h2>
      <p>Captioning by Discriminative Prompting (CDP) is based around predicting discriminative prompts for the set of similar clips. Our intuition is that it is easier to spot where a difference occurs, instead of fully captioning multiple clips with a captioner all at once.</p>
      <br />
      <p>This allows us to use one frozen single-clip pre-trained captioner, and only requires us to learn a lightweight and scalable network, CDPNet, which predicts the similarity between a video clip and a prompted caption in a shared embedding space. These similarities are then searched over to find the most unique caption. CDPNet is small enough to be conditioned on all the clips we want unique captions for.</p>

      <div class="figure-row">
        <div class="figure-part">
          <img src="figs/w1.png" alt="Part (b)">
          <p><em>(a) First, CDPNet is used to predict simiarities, which are searched over to predict a discriminative prompt for each video clip.</em></p>
        </div>
        <div class="figure-part">
          <img src="figs/w2.png" alt="Part (c)">
          <p><em>(b) Each video clip is then passed with its predicted prompt to a frozen single-clip captioner.</em></p>
        </div>
      </div>

      <p>We also acknowledge that all captioners have limitations. When CDPNet determines that the captioner cannot generate a unique caption for a given clip, we exploit the long-term nature of the video to advance temporally until it can.</p>
    </section>

    <!-- Benchmark Section -->
    <section id="benchmarks">
      <h2>Benchmarks</h2>
      <p>We curate sets of similar clips from two domains: egocentric videos from Ego4D and timeloop movies. Methods are evaluated by average video-to-text and text-to-video recall@1. We apply CDP to the SOTA captioner for each domain - LaViLa for egocentric and VideoLlama for movies, with significant improvements on each.</p>

      <!-- <table>
        <thead>
          <tr>
            <th>Method</th>
            <th>Accuracy (%)</th>
            <th>Speed (ms/frame)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Our Method</td>
            <td>95.6</td>
            <td>45</td>
          </tr>
          <tr>
            <td>Baseline Method A</td>
            <td>92.3</td>
            <td>50</td>
          </tr>
          <tr>
            <td>Baseline Method B</td>
            <td>91.0</td>
            <td>48</td>
          </tr>
        </tbody>
      </table>
      <p><em>Table 1: Benchmark results comparing our method with baseline approaches on the XYZ dataset.</em></p> -->
    </section>

    <!-- YouTube Video Embed Section -->
    <section id="video">
      <h2>Video Examples</h2>
      <p>Here are three examples of uninque captioning on three timeloop movies: "Groundhog Day", "Edge of Tomorrow" and "The Map of Tiny Perfect Things", followed by three egocentric examples. In all cases, note how every caption can uniquely identify the video-clip it was generated from.</p>
      <div class="video-container">
        <iframe src="https://www.youtube.com/embed/37Q1uA63kbU" frameborder="0" allowfullscreen></iframe>
      </div>
      <!-- <p><em>Watch the video presentation to learn more about our approach and experimental setup.</em></p> -->
    </section>

    <!-- GitHub Link Section
    <section id="github">
      <h2>Code Repository</h2>
      <p>Code/benchmarks/models are here:</p>
      <p><a href="https://github.com/your-repo-link" target="_blank">GitHub Repository</a></p>
    </section> -->

    <!-- BibTeX Entry Section -->
    <section id="bibtex">
      <h2>BibTeX Entry</h2>
      <pre>
        @inproceedings{perrett2024unique,
          title={It's Just Another Day: Unique Video Captioning by Discriminitave Prompting},
          author={Perrett, Toby and Han, Tengda and Damen, Dima and Zisserman, Andrew},
          booktitle={Asian Conference on Computer Vision},
          year={2024},
        }
      </pre>
    </section>
  </div>
</body>
</html>
