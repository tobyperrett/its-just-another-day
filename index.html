<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="description" content="It's Just Another Day">
  <title>It's Just Another Day</title>
  <link rel="stylesheet" href="styles.css">
</head>
<body>
  <div class="container">
    <!-- Paper Title Section -->
    <header>
      <h1>It's Just Another Day: Unique Video Captioning by Discriminitave Prompting</h1>
      <div class="authors">
        <h3>
          <a href="https://tobyperrett.github.io" target="_blank">Toby Perrett</a>, 
          <a href="https://tengdahan.github.io" target="_blank">Tengda Han</a>, 
          <a href="https://dimadamen.github.io" target="_blank">Dima Damen</a>, 
          <a href="https://www.robots.ox.ac.uk/~az/" target="_blank">Andrew Zisserman</a>, 
        </h3>
      </div>
    </header>

    <!-- Abstract Section -->
    <section id="introduction">
      <h2>Introduction</h2>
      <p>This paper investigates unique video captioning. We introduce a method, Captioning by Discriminitave Prompting (CDP) and challenging unique captioning benchmarks on Egocentric video and Timeloop movies.
      </p>
    </section>

    <!-- Figure Section -->
    <section id="problem statement">
      <h2>Problem statement</h2>
      <p>Given a set of similar video clips, the goal is to generate a caption for each, which is concise and focusses on what is unique to each clip. This should allow the caption to retrieve the video clip. Previous approaches have captioned clips independently, which means they are often assigned the same caption.</p>
      
      <!-- Part (a) -->
      <div class="figure-part-a">
        <img src="figure_a.png" alt="Part (a)" class="full-width-image">
        <p><em>(a) Overview of the entire architecture.</em></p>
      </div>
      
      <!-- Parts (b), (c), and (d) -->
      <div class="figure-row">
        <div class="figure-part">
          <img src="figure_b.png" alt="Part (b)">
          <p><em>(b) Close-up of the first component.</em></p>
        </div>
        <div class="figure-part">
          <img src="figure_c.png" alt="Part (c)">
          <p><em>(c) Performance comparison.</em></p>
        </div>
        <div class="figure-part">
          <img src="figure_d.png" alt="Part (d)">
          <p><em>(d) Example output segmentation.</em></p>
        </div>
      </div>


    </section>

    <!-- Introduction Section -->
    <section id="method">
      <h2>Method</h2>
      <p>Captioning by Discriminative Prompting (CDP) is based around predicting discriminative prompts for the set of similar clips. Our intuition is that it is easier to spot where a difference occurs, instead of fully captioning multiple clips with a captioner all at once.

        This allows us to use one frozen single-clip pre-trained captioner, and only requires us to learn a lightweight and scalable prompt-prediction network, CDPNet, which is small enough to be conditioned on all the clips we want unique captions for.
        
        We also acknowledge that all captioners have limitations. When CDPNet determines that the captioner cannot generate a unique caption for a given clip, we exploit the long-term nature of the video to advance temporally until it can.</p>
    </section>

    <!-- Benchmark Section -->
    <section id="benchmarks">
      <h2>Benchmarks</h2>
      <p>We curate sets of similar clips from two domains: Egocentric videos and Timeloop movies. Methods are evaluated by Average video-to-text and text-to-video recall @ 1. We apply CDP to the SOTA captioner for each domain - LaViLa for egocentric and VideoLlama for timeloop movies, with significant improvements on each.</p>

      <!-- <table>
        <thead>
          <tr>
            <th>Method</th>
            <th>Accuracy (%)</th>
            <th>Speed (ms/frame)</th>
          </tr>
        </thead>
        <tbody>
          <tr>
            <td>Our Method</td>
            <td>95.6</td>
            <td>45</td>
          </tr>
          <tr>
            <td>Baseline Method A</td>
            <td>92.3</td>
            <td>50</td>
          </tr>
          <tr>
            <td>Baseline Method B</td>
            <td>91.0</td>
            <td>48</td>
          </tr>
        </tbody>
      </table>
      <p><em>Table 1: Benchmark results comparing our method with baseline approaches on the XYZ dataset.</em></p> -->
    </section>

    <!-- YouTube Video Embed Section -->
    <section id="video">
      <h2>Video Examples</h2>
      <p>Here are three examples of uninque captioning on three timeloop movies: "Groundhog Day", "Edge of Tomorrow" and "The Map of Tiny Perfect Things", followed by three egocentric examples.</p>
      <div class="video-container">
        <iframe src="https://www.youtube.com/embed/YOUTUBE_VIDEO_ID" frameborder="0" allowfullscreen></iframe>
      </div>
      <!-- <p><em>Watch the video presentation to learn more about our approach and experimental setup.</em></p> -->
    </section>

    <!-- GitHub Link Section -->
    <section id="github">
      <h2>Code Repository</h2>
      <p>Code/benchmarks/models are here:</p>
      <p><a href="https://github.com/your-repo-link" target="_blank">GitHub Repository</a></p>
    </section>

    <!-- BibTeX Entry Section -->
    <section id="bibtex">
      <h2>BibTeX Entry</h2>
      <pre>
        @inproceedings{perrett2024unique,
          title={It's Just Another Day: Unique Video Captioning by Discriminitave Prompting},
          author={Perrett, Toby and Han, Tengda and Damen, Dima and Zisserman, Andrew},
          booktitle={Asian Conference on Computer Vision},
          year={2024},
        }
      </pre>
    </section>
  </div>
</body>
</html>
